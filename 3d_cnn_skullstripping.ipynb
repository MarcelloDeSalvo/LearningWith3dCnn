{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "957a329a"
      },
      "source": [
        "\n",
        "# 3D CNN | Human Brain Skullstripping with 3D UNet"
      ],
      "id": "957a329a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notebook example for \"Advanced Deep Learning Models and Methods for 3D Spatial Data\" course at Politecnico di Milano (Polimi) during the academic year 2023/2024, instructed by Professors Boracchi Giacomo, Magri Luca, Matteucci Matteo, and Melzi Simone.\n",
        "\n",
        "Author: Marcello De Salvo <br>\n",
        "Repo: https://github.com/MarcelloDeSalvo/LearningWith3dCnn\n"
      ],
      "metadata": {
        "id": "LWpg2jHattgM"
      },
      "id": "LWpg2jHattgM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hackathon Tasks\n",
        "\n",
        "*   Run the initial part of the script and get aquainted with visualization of 3D data. Open other images / change the sampling frequency of point clouds\n",
        "*   Implement the Unet3D as illustrated below\n",
        "*   Implement a modified variant of UNet3D which leverages residual connections in each block.\n",
        "*   Assess testing performance and compare the above models. Visualize the estimated masks\n",
        "\n"
      ],
      "metadata": {
        "id": "rP8li5ksaARf"
      },
      "id": "rP8li5ksaARf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c353e013"
      },
      "source": [
        "# Libraries"
      ],
      "id": "c353e013"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "962e60bd",
        "outputId": "032d47f8-1567-4b66-abe7-4201a5f60939"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nilearn\n",
            "  Downloading nilearn-0.10.3-py3-none-any.whl (10.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from nilearn) (1.3.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nilearn) (4.9.4)\n",
            "Requirement already satisfied: nibabel>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from nilearn) (4.0.2)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from nilearn) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from nilearn) (23.2)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from nilearn) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.25.0 in /usr/local/lib/python3.10/dist-packages (from nilearn) (2.31.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from nilearn) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from nilearn) (1.11.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nibabel>=4.0.0->nilearn) (67.7.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->nilearn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->nilearn) (2023.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.0->nilearn) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.0->nilearn) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.0->nilearn) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.0->nilearn) (2024.2.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->nilearn) (3.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.1.5->nilearn) (1.16.0)\n",
            "Installing collected packages: nilearn\n",
            "Successfully installed nilearn-0.10.3\n"
          ]
        }
      ],
      "source": [
        "# Utils\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Nibabel visualization and processing\n",
        "import nibabel as nib\n",
        "!pip install nilearn\n",
        "import nilearn\n",
        "from scipy.ndimage import zoom\n",
        "\n",
        "# Tensorflow and Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.optimizers import *\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "\n",
        "# sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")"
      ],
      "id": "962e60bd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08065710"
      },
      "outputs": [],
      "source": [
        "# Set random seed\n",
        "seed = 42\n",
        "tf.keras.utils.set_random_seed(seed)"
      ],
      "id": "08065710"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "333e1b0c"
      },
      "source": [
        "# Dataset\n",
        "We'll use this public dataset: http://preprocessed-connectomes-project.org/NFB_skullstripped/\n",
        "\n",
        "\n",
        "The dataset comprises information from 125 individuals aged between 21 and 45 years, encompassing a diverse range of clinical and subclinical psychiatric symptoms. Each participant's data includes:\n",
        "\n",
        "- Anonymized (de-faced for privacy) Structural T1-weighted image\n",
        "- Skull-stripped image\n",
        "- Brain mask\n",
        "\n",
        "The images are in NiFTI format (.nii.gz) with a resolution of 1 mm³."
      ],
      "id": "333e1b0c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4a01239"
      },
      "source": [
        "**What is a T1-weighted image?**\n",
        "\n",
        "A T1-weighted image is a type of magnetic resonance imaging (MRI) that highlights differences in tissue density, providing detailed anatomical information. It is particularly useful for visualizing brain structures since it provides crisp images, and shows fluids as dark.\n",
        "\n",
        "**What is a Nifti format?**\n",
        "\n",
        "NIfTI (Neuroimaging Informatics Technology Initiative) is a data format for the storage of Functional Magnetic Resonance Imaging (fMRI) and other medical images."
      ],
      "id": "d4a01239"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15f9f766"
      },
      "source": [
        "### Download and extract the dataset"
      ],
      "id": "15f9f766"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fc15f9b",
        "outputId": "a21fff6c-1b58-40a4-d177-d2ad5a181dd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1670M  100 1670M    0     0  11.8M      0  0:02:21  0:02:21 --:--:-- 13.4M\n"
          ]
        }
      ],
      "source": [
        "dataset_folder = './NFBS_Dataset'\n",
        "\n",
        "# Check if the folder already exists\n",
        "if not os.path.exists(dataset_folder):\n",
        "    # Step 1: Download the dataset\n",
        "    !curl -O https://fcp-indi.s3.amazonaws.com/data/Projects/RocklandSample/NFBS_Dataset.tar.gz\n",
        "\n",
        "    # Step 2: Extract the dataset\n",
        "    !tar -xf NFBS_Dataset.tar.gz\n",
        "\n",
        "    # Optionally, you can remove the tar file after extracting to save space\n",
        "    !rm NFBS_Dataset.tar.gz\n",
        "else:\n",
        "    print(f\"The folder '{dataset_folder}' already exists. Skipping download and extraction.\")\n"
      ],
      "id": "0fc15f9b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7a65944",
        "outputId": "dad7bec1-565b-4f33-b697-8e3e67b5390c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of patients (folders): 125\n"
          ]
        }
      ],
      "source": [
        "items = os.listdir(dataset_folder)\n",
        "\n",
        "patients = [item for item in items if os.path.isdir(os.path.join(dataset_folder, item))]\n",
        "num_patients = len(patients)\n",
        "\n",
        "print(\"Number of patients (folders):\", num_patients)"
      ],
      "id": "c7a65944"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3405483c"
      },
      "source": [
        "## Data exploration"
      ],
      "id": "3405483c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bce82228"
      },
      "source": [
        "We can load our NIfTI files using the **nibabel** library and utilize the **nilearn** library to process the MRIs and visualize them"
      ],
      "id": "bce82228"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95e7ac5b",
        "outputId": "c7ee4a50-7fd1-4583-ae94-6dc93f796ef4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "img shape -> (256, 256, 192)\n",
            "mask shape -> (256, 256, 192)\n"
          ]
        }
      ],
      "source": [
        "sample_filename = dataset_folder + '/A00028185/sub-A00028185_ses-NFB3_T1w.nii.gz'\n",
        "sample_filename_mask = dataset_folder + '/A00028185/sub-A00028185_ses-NFB3_T1w_brainmask.nii.gz'\n",
        "\n",
        "sample_img = nib.load(sample_filename)\n",
        "sample_img = sample_img.get_fdata()\n",
        "sample_mask = nib.load(sample_filename_mask)\n",
        "sample_mask = sample_mask.get_fdata()\n",
        "\n",
        "input_shape = sample_img.shape\n",
        "mask_shape = sample_mask.shape\n",
        "print(\"img shape ->\", input_shape)\n",
        "print(\"mask shape ->\", mask_shape)"
      ],
      "id": "95e7ac5b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09f49049"
      },
      "source": [
        "### 2D slices view"
      ],
      "id": "09f49049"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6b208c73"
      },
      "outputs": [],
      "source": [
        "from nilearn.plotting import view_img, plot_glass_brain, plot_anat, plot_epi\n",
        "from nilearn.image import load_img\n",
        "nilearn_img = load_img(sample_filename)\n",
        "nilearn_mask = load_img(sample_filename_mask)"
      ],
      "id": "6b208c73"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1b536724"
      },
      "outputs": [],
      "source": [
        "plot_anat(nilearn_img)"
      ],
      "id": "1b536724"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42885280"
      },
      "outputs": [],
      "source": [
        "plot_anat(nilearn_img, draw_cross=False, display_mode='z')"
      ],
      "id": "42885280"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e69cec03"
      },
      "outputs": [],
      "source": [
        "plot_anat(nilearn_mask, draw_cross=False, display_mode='z')"
      ],
      "id": "e69cec03"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b87c7cd2"
      },
      "outputs": [],
      "source": [
        "view_img(nilearn_mask, nilearn_img)"
      ],
      "id": "b87c7cd2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23e8d7b6"
      },
      "source": [
        "### 3D Pointcloud view"
      ],
      "id": "23e8d7b6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9b29bec6"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "def plot_3d_overlap(sample_img, sample_mask, density, alpha=0.2):\n",
        "\n",
        "    # adjust the rotation (just for visual purposes)\n",
        "    sample_img = np.swapaxes(sample_img, 1, 2)\n",
        "    sample_mask = np.swapaxes(sample_mask, 1, 2)\n",
        "\n",
        "    # Invert Z-axis (just for visual purposes)\n",
        "    sample_img = sample_img[::-1, :, ::-1]\n",
        "    sample_mask = sample_mask[::-1, :, ::-1]\n",
        "\n",
        "    # Brain and Gt\n",
        "    _sample_mask = np.where(sample_mask > 0.5, 1, 0)\n",
        "    gt_indices = np.argwhere(_sample_mask > 0)\n",
        "    brain_indices =  np.argwhere(sample_img > 200) # remove noise\n",
        "\n",
        "    # Randomly sample points based on the specified density\n",
        "    brain_indices = brain_indices[np.random.choice(len(brain_indices), density, replace=False)]\n",
        "    gt_indices = gt_indices[np.random.choice(len(gt_indices), density, replace=False)]\n",
        "\n",
        "    # Get matrix values for color mapping\n",
        "    brain_colors = sample_img[brain_indices[:, 0], brain_indices[:, 1], brain_indices[:, 2]]\n",
        "    gt_colors = sample_mask[gt_indices[:, 0], gt_indices[:, 1], gt_indices[:, 2]]\n",
        "\n",
        "\n",
        "    # Create 3D scatter plot for the brain mask\n",
        "    brain_scatter = go.Scatter3d(\n",
        "        x=brain_indices[:, 0],\n",
        "        y=brain_indices[:, 1],\n",
        "        z=brain_indices[:, 2],\n",
        "        mode='markers',\n",
        "        marker=dict(\n",
        "            size=1,\n",
        "            opacity=0.3,\n",
        "            color=brain_colors,\n",
        "            colorscale='Viridis',\n",
        "            cmin=np.min(brain_colors),\n",
        "            cmax=np.max(brain_colors),\n",
        "        ),\n",
        "        name='Head'\n",
        "    )\n",
        "\n",
        "    # Create 3D scatter plot for the gt\n",
        "    gt_scatter = go.Scatter3d(\n",
        "        x=gt_indices[:, 0],\n",
        "        y=gt_indices[:, 1],\n",
        "        z=gt_indices[:, 2],\n",
        "        mode='markers',\n",
        "        marker=dict(\n",
        "            size=1.5,\n",
        "            color=gt_colors,\n",
        "            colorscale='YlOrRd',\n",
        "            cmin=np.min(gt_colors),\n",
        "            cmax=np.max(gt_colors),\n",
        "        ),\n",
        "        name='Ground truth'\n",
        "    )\n",
        "\n",
        "    # Create figure\n",
        "    fig = go.Figure(data=[brain_scatter, gt_scatter])\n",
        "\n",
        "    # Set layout properties\n",
        "    fig.update_layout(\n",
        "        scene=dict(\n",
        "            xaxis_title='X',\n",
        "            yaxis_title='Y',\n",
        "            zaxis_title='Z',\n",
        "        ),\n",
        "        title='3D Plot - Brain Mask Point Cloud'\n",
        "    )\n",
        "\n",
        "    # Show figure\n",
        "    fig.show()\n",
        "\n",
        "plot_3d_overlap(sample_img, sample_mask, density=50000)"
      ],
      "id": "9b29bec6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39ed2c76"
      },
      "source": [
        "# Problem definiton\n",
        "\n",
        "The objective of this task is to create a 3D model for binary semantic segmentation, where each voxel in the volumetric data is assigned one of the following labels:\n",
        "0. Background (other anatomical tissues such as bones and flesh)\n",
        "1. Brain tissue\n",
        "\n",
        "This procedure (called \"skull stripping\") that recognizes and separates the brain and surrounding tissues from the MRI data, is essential for many applications, such as medical picture analysis and neuroscience research."
      ],
      "id": "39ed2c76"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed578a7a",
        "outputId": "9788fda7-6daf-415b-e536-59f2780a65ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resampled input shape:  (64, 64, 48)\n"
          ]
        }
      ],
      "source": [
        "config = {\n",
        "    'input_shape': input_shape,\n",
        "    'dataset_folder': dataset_folder,\n",
        "    'batch_size': 3,\n",
        "    'num_classes': 1,  # Binary segmentation\n",
        "    'num_channels': 1, # Just one input channel for the single t1w modality\n",
        "    'validation_split': 0.2,\n",
        "    'test_split': 0.2,\n",
        "    'target_resolution': (4,4,4) # We'll downsample the data from 1 mm³ to 4 mm³ (You can also try with 2 mm³ but it takes longer to train!)\n",
        "}\n",
        "\n",
        "config['input_shape'] = tuple(int(dim / resolution) for dim, resolution in zip(config['input_shape'], config['target_resolution']))\n",
        "print('Resampled input shape: ', config['input_shape'])"
      ],
      "id": "ed578a7a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b23e289f"
      },
      "source": [
        "# Loss function\n",
        "## Dice coefficient <br>\n",
        "The Dice coefficient (also known as the Sørensen-Dice coefficient) is used to measure the similarity or overlap between two sets.\n",
        "\n",
        "In this context, it's used to measure the similarity between the ground truth binary mask (y_true) and the predicted binary mask (y_pred) for a specific class. This measure ranges from 0 to 1 where a Dice coefficient of 1 denotes perfect and complete overlap. <br>\n",
        "The Dice coefficient can be calculated as: <br>\n",
        "$$\n",
        "\\Large\n",
        "\\text{Dice Coefficient} = \\frac{2 \\times |X \\cap Y|}{|X| + |Y|}\n",
        "$$\n",
        "In our case this translates into:\n",
        "- Intersection: This is the sum of the element-wise multiplication of the ground truth mask and the predicted mask.<br>\n",
        "- X or Area of y_true: This is the sum of all non-zero values in the ground truth mask, representing the total number of pixels that belong to the class in the ground truth.\n",
        "- Y or Area of y_pred: This is the sum of all non-zero values in the predicted mask, representing the total number of pixels that the model predicted as belonging to the class.\n",
        "\n",
        "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSrnfpehrVZMJLjRDVUWxEZ9_pW0RYUlkdhlw&usqp=CAU\" alt=\"Image 1\" style=\"width:400px; height:200px\"/>"
      ],
      "id": "b23e289f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "545ed4ee"
      },
      "outputs": [],
      "source": [
        "def dice_loss(smooth=1e-5):\n",
        "    '''\n",
        "    This loss function is known as the Soft Dice loss because we directly use the predicted probabilities\n",
        "    instead of thresholding and converting them into a binary mask.\n",
        "    '''\n",
        "    def loss(y_true, y_pred):\n",
        "        return 1 - dice(y_true, y_pred, smooth)\n",
        "    return loss\n",
        "\n",
        "def dice(y_true, y_pred, smooth=1e-5):\n",
        "    '''\n",
        "    Soft dice\n",
        "    '''\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    union = K.sum(y_pred_f) + K.sum(y_true_f)\n",
        "    return (2. * intersection + smooth) / (union + smooth)\n",
        "\n",
        "def dice_bce_loss(alpha=0.5, smooth=1e-5):\n",
        "    '''\n",
        "    Combined loss: Weighted sum of Dice loss and Binary Crossentropy loss\n",
        "    Combining a Dice loss with Binary Cross-Entropy (BCE) is another frequently employed strategy\n",
        "    '''\n",
        "    def loss(y_true, y_pred):\n",
        "        dice_loss_value = dice_loss(smooth)(y_true, y_pred)\n",
        "        bce_loss_value = binary_crossentropy(y_true, y_pred)\n",
        "\n",
        "        combined_loss = alpha * dice_loss_value + (1 - alpha) * bce_loss_value\n",
        "        return combined_loss\n",
        "    return loss"
      ],
      "id": "545ed4ee"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "484c1e85"
      },
      "source": [
        "## Other useful metrics for 3D data"
      ],
      "id": "484c1e85"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5cb83fca"
      },
      "outputs": [],
      "source": [
        "def precision(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def sensitivity(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    return true_positives / (possible_positives + K.epsilon())\n",
        "\n",
        "def specificity(y_true, y_pred):\n",
        "    true_negatives = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))\n",
        "    possible_negatives = K.sum(K.round(K.clip(1-y_true, 0, 1)))\n",
        "    return true_negatives / (possible_negatives + K.epsilon())\n",
        "\n",
        "def iou_coeff(y_true, y_pred, threshold=0.5):\n",
        "    '''\n",
        "    IoU (Intersection over Union) coefficient\n",
        "    - threshold: threshold for prediction binaryzation, set low probabilities to 0\n",
        "    '''\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    y_pred_f = K.greater(y_pred_f, threshold)\n",
        "    y_pred_f = K.cast(y_pred_f, dtype='float32') # Becomes binary\n",
        "\n",
        "    # Intersection and Union\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    union = K.sum(K.maximum(y_true_f, y_pred_f))\n",
        "\n",
        "    # Calculate IoU (Intersection over Union)\n",
        "    iou = (intersection + K.epsilon()) / (union + K.epsilon())\n",
        "\n",
        "    return iou"
      ],
      "id": "5cb83fca"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3717c781"
      },
      "outputs": [],
      "source": [
        "_metrics = ['accuracy', precision, sensitivity, specificity, dice, iou_coeff]"
      ],
      "id": "3717c781"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06b23def"
      },
      "source": [
        "# Data Generator\n",
        "## Data Loader\n",
        "Loading all data into memory is not a good idea since the data are too big to fit in.<br>\n",
        "So we will create a DataGenerators class to load data on the fly as explained [here](https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly)\n",
        "\n",
        "## Preprocessing\n",
        "1. First, we'll reduce the resolution of each image by resampling it. (Ex: from 0.1 mm$^3$ to 0.2 mm$^3$)\n",
        "\n",
        "2. Second, we will z-score normalization to normalize the MRI data's histogram frequencies."
      ],
      "id": "06b23def"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9e96d202"
      },
      "outputs": [],
      "source": [
        "dataset_ids = []\n",
        "\n",
        "for patient in os.listdir(dataset_folder):\n",
        "    dataset_ids.append(patient)\n",
        "\n",
        "dataset_ids.sort()\n",
        "print('Size of the dataset: ', len(dataset_ids))\n",
        "\n",
        "# Splitting\n",
        "train_test_ids, val_ids = train_test_split(dataset_ids,test_size=config['validation_split'], random_state=seed)\n",
        "train_ids, test_ids = train_test_split(train_test_ids,test_size=config['test_split'], random_state=seed)\n",
        "\n",
        "print('Size of the training set: ', len(train_ids))\n",
        "print('Size of the validation set: ', len(val_ids))\n",
        "print('Size of the test set: ', len(test_ids))\n",
        "print(test_ids)"
      ],
      "id": "9e96d202"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "be64e424"
      },
      "outputs": [],
      "source": [
        "class DataGenerator(tf.keras.utils.Sequence):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self, list_IDs, dim = config['input_shape'], dataset_folder = config['dataset_folder'], batch_size = config['batch_size'],\n",
        "                 n_channels = config['num_channels'], n_class = config['num_classes'], target_resolution=config['target_resolution'],  shuffle=True):\n",
        "        'Initialization'\n",
        "        self.dim = dim\n",
        "        self.dataset_folder = dataset_folder\n",
        "        self.batch_size = batch_size\n",
        "        self.list_IDs = list_IDs\n",
        "        self.n_channels = n_channels\n",
        "        self.shuffle = shuffle\n",
        "        self.n_class = n_class\n",
        "        self.target_resolution = target_resolution\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "\n",
        "        # Find list of IDs\n",
        "        batch_ids = [self.list_IDs[k] for k in indexes]\n",
        "\n",
        "        # Generate data\n",
        "        X, Y = self.__data_generation(batch_ids)\n",
        "        return X, Y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indexes after each epoch'\n",
        "        self.indexes = np.arange(len(self.list_IDs))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def res_nifti(self, image):\n",
        "        'Resamples the nifti image to a target resolution'\n",
        "        voxel_dims = image.header.get_zooms() # Automatically retrieves the original resolution from the nifti file\n",
        "        scale_factors =  [current_dim / target_dim for current_dim, target_dim in zip(voxel_dims, self.target_resolution)]\n",
        "        resampled_data = zoom(image.get_fdata(), scale_factors, order=0, mode='nearest')\n",
        "\n",
        "        return resampled_data\n",
        "\n",
        "    def normalize(self, arr, mode='zscore'):\n",
        "        # min max normalization\n",
        "        if mode == 'minmax':\n",
        "            return (arr - arr.min()) / (arr.max() - arr.min())\n",
        "        # zscore normalization\n",
        "        elif mode == 'zscore':\n",
        "            return (arr - arr.mean()) / arr.std()\n",
        "        else:\n",
        "            print(\"ERROR: Unknown normalization mode\")\n",
        "            return arr\n",
        "\n",
        "    def __data_generation(self, batch_ids):\n",
        "        'Generates data containing batch_size samples'\n",
        "\n",
        "        # Initialization\n",
        "        X = np.zeros((self.batch_size,*(self.dim),self.n_channels))\n",
        "        Y = np.zeros((self.batch_size,*(self.dim),self.n_class))\n",
        "\n",
        "        # Generate data\n",
        "        for i, id in enumerate(batch_ids):\n",
        "            case_path = os.path.join(self.dataset_folder, id)\n",
        "\n",
        "            # Original Image Preprocessing\n",
        "            image_path = os.path.join(case_path, f'sub-{id}_ses-NFB3_T1w.nii.gz')\n",
        "            image = nib.load(image_path)\n",
        "            image_data = self.res_nifti(image)\n",
        "            image_data = self.normalize(image_data)\n",
        "\n",
        "            # Mask preprocessing\n",
        "            mask_path = os.path.join(case_path, f'sub-{id}_ses-NFB3_T1w_brainmask.nii.gz')\n",
        "            mask = nib.load(mask_path)\n",
        "            mask_data = self.res_nifti(mask)\n",
        "            mask_data = np.where(mask_data > 0.5, 1, 0)\n",
        "\n",
        "            if (self.n_class == 1): mask_data = np.expand_dims(mask_data, axis=-1)\n",
        "            else: mask_data = tf.keras.utils.to_categorical(mask_data,self.n_class)\n",
        "\n",
        "            # Stacking\n",
        "            X[i] = np.stack((image_data,)*self.n_channels, axis=-1)\n",
        "            Y[i] = mask_data\n",
        "\n",
        "        return X, Y"
      ],
      "id": "be64e424"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "914e0cb2"
      },
      "outputs": [],
      "source": [
        "# Datasets Initialization\n",
        "training_generator = DataGenerator(train_ids)\n",
        "valid_generator = DataGenerator(val_ids)\n",
        "test_generator = DataGenerator(test_ids)"
      ],
      "id": "914e0cb2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ec9c7df"
      },
      "source": [
        "## Sanity check\n",
        "Here we verify whether the image has been resampled correctly, ensuring that there are no errors in the preprocessing of the DataGenerator by checking the output shapes and by visualizing a preprocesed sample image."
      ],
      "id": "9ec9c7df"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04d313f1"
      },
      "outputs": [],
      "source": [
        "X,Y = training_generator.__getitem__(index=0) # Fetching the first batch\n",
        "# Check if shapes match expectations\n",
        "assert X.shape == (config['batch_size'], *(config['input_shape']), config['num_channels'])\n",
        "assert Y.shape == (config['batch_size'], *(config['input_shape']), config['num_classes'])\n",
        "\n",
        "# Check if Y is one-hot encoded\n",
        "print(\"Unique values in Y: \", np.unique(Y))\n",
        "\n",
        "# Take the slice in the middle\n",
        "slice_idx = config['input_shape'][-1] // 2\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(20, 20))\n",
        "plt.subplot(1,5,1)\n",
        "plt.imshow(np.rot90(X[0,:,:,slice_idx,0], k=-1), cmap='gray')\n",
        "plt.title('T1 image')\n",
        "plt.subplot(1,5,2)\n",
        "plt.imshow(np.rot90(Y[0,:,:,slice_idx,0], k=-1), cmap='gray')\n",
        "plt.title('Ground truth')\n",
        "plt.show()"
      ],
      "id": "04d313f1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1783744c"
      },
      "source": [
        "# Model | 3D U-Net\n",
        "The success of U-Net has led to the development of derivative architectures by other researchers, including those employing 3D convolutions: [Çiçek et al. in 2016](https://arxiv.org/abs/1606.06650) showed that 3D architectures can achieve comprehensive 3D segmentation with minimal annotated slices from the same volume. Building on this progress, on the same year, [Milletari et al.](https://arxiv.org/abs/1606.04797) proposed a 3D version of the U-Net, trained using the Dice Coefficient.\n",
        "\n",
        "## 3D convolutions\n",
        "3D convolutions expand the convolutional operation into an additional dimension, making them perfect a perfect fit for volumetric and temporal data (e.g. [with videos](https://www.tensorflow.org/tutorials/video/video_classification)).\n",
        "\n",
        "Within the field of medical imaging, specifically in applications such as this one, we can use **voxels** to represent three-dimensional datasets like those from CT and MRI scans. With each voxel holding the signal value, these provide an intricate depiction of the underlying anatomy. For this reasons, 3D U-Net topologies have demonstrated excellent performance in a variety of biomedical applications.\n",
        "![](https://i.imgur.com/TYMETaw.gif)\n",
        "\n",
        "## 3D Unet architecture\n",
        "To accomplish this task we could design something like this:\n",
        "\n",
        "![](https://i.imgur.com/gbzLYgG.png)"
      ],
      "id": "1783744c"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keras provides already some useful layers that can handle 3D images such as [Conv3D](https://keras.io/api/layers/convolution_layers/convolution3d/), [MaxPooling3D](https://keras.io/api/layers/pooling_layers/max_pooling3d/) and [UpSampling3D](https://keras.io/api/layers/reshaping_layers/up_sampling3d/) layers, and you can also use [BatchNormalization](https://keras.io/api/layers/normalization_layers/batch_normalization/). <br>\n",
        "*Note that with 'UpConv3D' we are referring to an operation involving an UpSampling3D layer followed by a Conv3D layer.*"
      ],
      "metadata": {
        "id": "KcBq5pQPvgWP"
      },
      "id": "KcBq5pQPvgWP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f27fc1e6"
      },
      "outputs": [],
      "source": [
        "# TODO: implement these blocks according to the architecture shown above\n",
        "\n",
        "# This is the C block shown in the above architecture\n",
        "def conv_block(x, filters, kernel_size=3, padding='same', activation='relu', kernel_initializer='he_normal'):\n",
        "    # TODO\n",
        "    return x\n",
        "\n",
        "# This is the CDC block shown above\n",
        "def double_conv_block(x, filters, kernel_size=3, padding='same', activation='relu', kernel_initializer='he_normal'):\n",
        "    # TODO\n",
        "    return x\n",
        "\n",
        "# This is the encoder block (CDC + MaxPooling3D) that returns the output and a skip connection\n",
        "def encoder_block(input, num_filter):\n",
        "    # TODO\n",
        "    return out, cdc\n",
        "\n",
        "# This is the decoder block (UpConv3D + Skip connection concatenation + CDC)\n",
        "# Simply concatenate the skip connections with the UpConv3D layer output\n",
        "def decoder_block(input, skip, num_filter):\n",
        "    # TODO\n",
        "    return cdc"
      ],
      "id": "f27fc1e6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b129ea89"
      },
      "outputs": [],
      "source": [
        "# Assemble the encoder, decoder and all the skip connections in a UNet3D\n",
        "def UNet3D(in_shape, in_channels, num_classes):\n",
        "    inputs = Input(shape=(in_shape[0], in_shape[1], in_shape[2], in_channels))\n",
        "\n",
        "    ## -------------Encoder--------------\n",
        "    # TODO\n",
        "\n",
        "    ## -----------Bottleneck-------------\n",
        "    # TODO\n",
        "\n",
        "    ## -------------Decoder--------------\n",
        "    # TODO\n",
        "\n",
        "    outputs = Conv3D(1, kernel_size=1, activation='sigmoid')(d3) # Final sigmoid activation\n",
        "    return Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model = UNet3D(config['input_shape'], config['num_channels'], config['num_classes'])"
      ],
      "id": "b129ea89"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8d15162c"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ],
      "id": "8d15162c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a846dfa7"
      },
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=15),\n",
        "    ]"
      ],
      "id": "a846dfa7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1d21738"
      },
      "outputs": [],
      "source": [
        "steps = len(train_ids) // config['batch_size']\n",
        "val_steps = len(val_ids) // config['batch_size']\n",
        "\n",
        "# Training will take a while! (In the solution: 15min for 4 mm³, 30 for 2 mm³)\n",
        "model.compile(loss=dice_loss(), optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), metrics = _metrics)\n",
        "history = model.fit(training_generator,epochs=15, steps_per_epoch=steps, callbacks= callbacks, validation_data=valid_generator, validation_steps=val_steps)"
      ],
      "id": "a1d21738"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59413865"
      },
      "outputs": [],
      "source": [
        "model.save(\"skull_stripping_net.h5\")"
      ],
      "id": "59413865"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2574fd2"
      },
      "source": [
        "## Results"
      ],
      "id": "e2574fd2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91290150"
      },
      "outputs": [],
      "source": [
        "_history = model.history.history\n",
        "epoch = range(len(_history['loss']))"
      ],
      "id": "91290150"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dc9eb9a6"
      },
      "outputs": [],
      "source": [
        "def plot_metrics(ax, metric, val_metric, label, val_label):\n",
        "    ax.plot(epoch, metric, 'b', label=label)\n",
        "    ax.plot(epoch, val_metric, 'r', label=val_label)\n",
        "    ax.legend()\n",
        "\n",
        "metrics = ['accuracy', 'loss', 'precision', 'dice', 'iou_coeff']\n",
        "labels = ['Training Accuracy', 'Training Loss', 'Precision', 'Dice', 'IoU']\n",
        "\n",
        "f, ax = plt.subplots(1, 5, figsize=(16, 5))\n",
        "\n",
        "for i in range(5):\n",
        "    plot_metrics(ax[i], _history[metrics[i]], _history[f'val_{metrics[i]}'], labels[i], f'Validation {labels[i]}')\n",
        "\n",
        "plt.show()"
      ],
      "id": "dc9eb9a6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcae3f52"
      },
      "source": [
        "## Evaluation"
      ],
      "id": "dcae3f52"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4885bfe2"
      },
      "outputs": [],
      "source": [
        "print(\"Evaluate on test data\")\n",
        "results = model.evaluate(test_generator, callbacks=callbacks)\n",
        "print(\"test evaluation metrics:\", results)"
      ],
      "id": "4885bfe2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0f574e33"
      },
      "outputs": [],
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "headers = ['Metric', 'Value']\n",
        "data = [\n",
        "    ['Loss', results[0]],\n",
        "    ['Accuracy', results[1]],\n",
        "    ['Precision', results[2]],\n",
        "    ['Sensitivity', results[3]],\n",
        "    ['Specificity', results[4]],\n",
        "    ['Dice', results[5]],\n",
        "    ['IoU', results[6]]\n",
        "]\n",
        "\n",
        "# Print the table\n",
        "table = tabulate(data, headers, tablefmt='pretty')\n",
        "print(table)"
      ],
      "id": "0f574e33"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52ff1307"
      },
      "outputs": [],
      "source": [
        "def showPredictsById(generator, model, image_id, alpha=0.4, axis=1):\n",
        "    # Get data for the specified image_id from the generator\n",
        "    processed_image, ground_truth = generator.__getitem__(image_id)\n",
        "\n",
        "    # Make predictions using the model\n",
        "    prediction = model.predict(processed_image)\n",
        "\n",
        "    if (axis == 2): rotation = -1\n",
        "    else: rotation = 0\n",
        "\n",
        "    # Display the images\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    plt.subplot(1, 4, 1)\n",
        "    plt.imshow(np.rot90(np.take(processed_image[0,:,:,:,0], slice_idx, axis=axis), k=rotation), cmap='gray')\n",
        "    plt.title(\"Original Image\")\n",
        "\n",
        "    plt.subplot(1, 4, 2)\n",
        "    plt.imshow(np.rot90(np.take(ground_truth[0,:,:,:,0], slice_idx, axis=axis), k=rotation), cmap='gray')\n",
        "    plt.title(\"Ground Truth\")\n",
        "\n",
        "    plt.subplot(1, 4, 3)\n",
        "    plt.imshow(np.rot90(np.take(prediction[0,:,:,:,0], slice_idx, axis=axis), k=rotation), cmap='gray')\n",
        "    plt.title(\"Prediction\")\n",
        "\n",
        "    plt.subplot(1, 4, 4)\n",
        "    plt.imshow(np.rot90(np.take(processed_image[0,:,:,:,0], slice_idx, axis=axis), k=rotation), cmap='gray')\n",
        "    plt.imshow(np.rot90(np.take(prediction[0,:,:,:,0], slice_idx, axis=axis), k=rotation), cmap='jet', alpha=alpha)\n",
        "    plt.title(\"Overlap\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# To plot the pre-processed data we can use our custom test generator with batch_size equal to 1\n",
        "test_generator = DataGenerator(list_IDs=test_ids, batch_size=1)\n",
        "\n",
        "for index in range(len(test_generator)):\n",
        "    random_axis = np.random.randint(0, 3)\n",
        "    showPredictsById(test_generator, model, index, axis=random_axis)"
      ],
      "id": "52ff1307"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c22bc592"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "def plot_3d_overlap(generator, model, image_id, alpha=0.1, density=10000, threshold=200):\n",
        "    processed_image, ground_truth = generator.__getitem__(image_id)\n",
        "\n",
        "    # Brain and Gt\n",
        "    gt_volume = ground_truth[0, :, :, :, 0]\n",
        "    brain_volume = processed_image[0, :, :, :, 0]\n",
        "\n",
        "    # Prediction\n",
        "    prediction = model.predict(processed_image)\n",
        "    prediction_volume = prediction[0, :, :, :, 0]\n",
        "    prediction_volume = np.where(prediction_volume > 0.5, 1, 0)\n",
        "\n",
        "    # Fix rotation and swap axis (just for visual purposes)\n",
        "    brain_volume = np.swapaxes(brain_volume, 1, 2)\n",
        "    gt_volume = np.swapaxes(gt_volume, 1, 2)\n",
        "    prediction_volume = np.swapaxes(prediction_volume, 1, 2)\n",
        "\n",
        "    # Invert axis (just for visual purposes)\n",
        "    brain_volume = brain_volume[::-1, :, ::-1]\n",
        "    gt_volume = gt_volume[::-1, :, ::-1]\n",
        "    prediction_volume = prediction_volume[::-1, :, ::-1]\n",
        "\n",
        "    # Get Indices\n",
        "    gt_indices = np.argwhere(gt_volume > 0)\n",
        "    brain_indices =  np.argwhere(brain_volume > threshold) # remove noise\n",
        "    prediction_indices = np.argwhere(prediction_volume > 0)\n",
        "\n",
        "    # Randomly sample points based on the specified density\n",
        "    brain_indices = brain_indices[np.random.choice(len(brain_indices), density, replace=False)]\n",
        "\n",
        "    # Get matrix values for color mapping\n",
        "    brain_colors = brain_volume[brain_indices[:, 0], brain_indices[:, 1], brain_indices[:, 2]]\n",
        "\n",
        "    # Create 3D scatter plot for the brain mask\n",
        "    brain_scatter = go.Scatter3d(\n",
        "        x=brain_indices[:, 0],\n",
        "        y=brain_indices[:, 1],\n",
        "        z=brain_indices[:, 2],\n",
        "        mode='markers',\n",
        "        marker=dict(\n",
        "            size=1,\n",
        "            opacity=0.2,\n",
        "            color=brain_colors,\n",
        "            colorscale='Viridis',\n",
        "            cmin=np.min(brain_colors),\n",
        "            cmax=np.max(brain_colors),\n",
        "        ),\n",
        "        name='Brain Mask Point Cloud'\n",
        "    )\n",
        "\n",
        "    # Create 3D scatter plot for the gt\n",
        "    gt_scatter = go.Scatter3d(\n",
        "        x=gt_indices[:, 0],\n",
        "        y=gt_indices[:, 1],\n",
        "        z=gt_indices[:, 2],\n",
        "        mode='markers',\n",
        "        marker=dict(\n",
        "            size=2,\n",
        "            color='yellow',\n",
        "            opacity=alpha\n",
        "        ),\n",
        "        name='Ground truth'\n",
        "    )\n",
        "\n",
        "    # Create 3D scatter plot for the prediction\n",
        "    prediction_scatter = go.Scatter3d(\n",
        "        x=prediction_indices[:, 0],\n",
        "        y=prediction_indices[:, 1],\n",
        "        z=prediction_indices[:, 2],\n",
        "        mode='markers',\n",
        "        marker=dict(\n",
        "            size=1,\n",
        "            color='red',\n",
        "            opacity=alpha\n",
        "        ),\n",
        "        name='Prediction'\n",
        "    )\n",
        "\n",
        "    # Create figure\n",
        "    fig = go.Figure(data=[brain_scatter, gt_scatter, prediction_scatter])\n",
        "\n",
        "    # Set layout properties\n",
        "    fig.update_layout(\n",
        "        scene=dict(\n",
        "            xaxis_title='X',\n",
        "            yaxis_title='Y',\n",
        "            zaxis_title='Z',\n",
        "        ),\n",
        "        title='3D Plot - Brain Mask Point Cloud',\n",
        "        annotations=[\n",
        "        dict(\n",
        "            text='You can switch on and off the masks by clicking',\n",
        "            x=1.05,\n",
        "            y=1,\n",
        "            align='right',\n",
        "            font=dict(family='Arial', size=12, color='black'),\n",
        "        ),\n",
        "    ],\n",
        "    )\n",
        "\n",
        "    # Show figure\n",
        "    fig.show()\n",
        "\n",
        "random_index = np.random.randint(0, len(test_generator) - 1)\n",
        "plot_3d_overlap(test_generator, model, random_index, density=30000, threshold=0.2)"
      ],
      "id": "c22bc592"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Variant | 3D U-Net with Residual Skip connections"
      ],
      "metadata": {
        "id": "Pz5UFgCYmzDk"
      },
      "id": "Pz5UFgCYmzDk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the residual block, you can substitute every CDC block with something like this:\n",
        "\n",
        "![](https://i.imgur.com/6n0QNCI.png)\n",
        "\n",
        "where a shortcut connection (identity mapping) is processed in parallel with a 1x1x1 3d convolutional block that adjusts shapes and is then added ([Add layer](https://keras.io/api/layers/merging_layers/add/)) before applying the final ReLU."
      ],
      "metadata": {
        "id": "IHkKjjYynICJ"
      },
      "id": "IHkKjjYynICJ"
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Implement a modified variant of UNet3D which leverages residual connections in each block.\n",
        "\n",
        "# This is the residual block showed in the diagram\n",
        "def residual_block(res, filters, kernel_size=3, padding='same', activation='relu', kernel_initializer='he_normal'):\n",
        "    # TODO\n",
        "    return x\n",
        "\n",
        "# This is the encoder block (CDC + MaxPooling3D) that returns the output and a skip connection\n",
        "def res_encoder_block(input, num_filter):\n",
        "    # TODO\n",
        "    return out, cdc\n",
        "\n",
        "# This is the decoder block (UpConv3D + Skip connection concatenation + CDC)\n",
        "# Simply concatenate the skip connections with the UpConv3D layer output\n",
        "def res_decoder_block(input, skip, num_filter):\n",
        "    # TODO\n",
        "    return cdc"
      ],
      "metadata": {
        "id": "yxOhqZGWnBF-"
      },
      "id": "yxOhqZGWnBF-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assemble the encoder, decoder and all the skip connections in a ResUNet3D\n",
        "def ResUNet3D(in_shape, in_channels, num_classes):\n",
        "    inputs = Input(shape=(in_shape[0], in_shape[1], in_shape[2], in_channels))\n",
        "\n",
        "    ## -------------Encoder--------------\n",
        "    # TODO\n",
        "\n",
        "    ## -----------Bottleneck-------------\n",
        "    # TODO\n",
        "\n",
        "    ## -------------Decoder--------------\n",
        "    # TODO\n",
        "\n",
        "    outputs = Conv3D(1, kernel_size=1, activation='sigmoid')(d3) # Final sigmoid activation\n",
        "    return Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "res_model = ResUNet3D(config['input_shape'], config['num_channels'], config['num_classes'])"
      ],
      "metadata": {
        "id": "sZdOmGDOslP3"
      },
      "id": "sZdOmGDOslP3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res_model.summary()"
      ],
      "metadata": {
        "id": "_-xq8Qqv04Wf"
      },
      "id": "_-xq8Qqv04Wf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res_model.compile(loss=dice_loss(), optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), metrics = _metrics)\n",
        "history = res_model.fit(training_generator,epochs=15, steps_per_epoch=steps, callbacks= callbacks, validation_data=valid_generator, validation_steps=val_steps)"
      ],
      "metadata": {
        "id": "kdREjNQ27uu1"
      },
      "id": "kdREjNQ27uu1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res_model.save(\"residual_skull_stripping_net.h5\")"
      ],
      "metadata": {
        "id": "2MUQ1fXVmkQL"
      },
      "id": "2MUQ1fXVmkQL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results"
      ],
      "metadata": {
        "id": "qNV2KDcZ7FKh"
      },
      "id": "qNV2KDcZ7FKh"
    },
    {
      "cell_type": "code",
      "source": [
        "_res_history = res_model.history.history\n",
        "epoch = range(len(_res_history['loss']))"
      ],
      "metadata": {
        "id": "5CdS2BFV6JKf"
      },
      "id": "5CdS2BFV6JKf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f, ax = plt.subplots(1, 5, figsize=(16, 5))\n",
        "\n",
        "for i in range(5):\n",
        "    plot_metrics(ax[i], _res_history[metrics[i]], _res_history[f'val_{metrics[i]}'], labels[i], f'Validation {labels[i]}')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "e6WOzkur6WUJ"
      },
      "id": "e6WOzkur6WUJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "DrxI9gs97CMD"
      },
      "id": "DrxI9gs97CMD"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Evaluate on test data\")\n",
        "results = res_model.evaluate(test_generator, callbacks=callbacks)\n",
        "print(\"test evaluation metrics:\", results)"
      ],
      "metadata": {
        "id": "hyEOrJne6c2Q"
      },
      "id": "hyEOrJne6c2Q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "headers = ['Metric', 'Value']\n",
        "data = [\n",
        "    ['Loss', results[0]],\n",
        "    ['Accuracy', results[1]],\n",
        "    ['Precision', results[2]],\n",
        "    ['Sensitivity', results[3]],\n",
        "    ['Specificity', results[4]],\n",
        "    ['Dice', results[5]],\n",
        "    ['IoU', results[6]]\n",
        "]\n",
        "\n",
        "# Print the table\n",
        "table = tabulate(data, headers, tablefmt='pretty')\n",
        "print(table)"
      ],
      "metadata": {
        "id": "j873HAK06kyD"
      },
      "id": "j873HAK06kyD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To plot the pre-processed data we can use our custom test generator with batch_size equal to 1\n",
        "test_generator = DataGenerator(list_IDs=test_ids, batch_size=1)\n",
        "\n",
        "for index in range(len(test_generator)):\n",
        "    random_axis = np.random.randint(0, 3)\n",
        "    showPredictsById(test_generator, res_model, index, axis=random_axis)"
      ],
      "metadata": {
        "id": "SU3O4XOl6ox1"
      },
      "id": "SU3O4XOl6ox1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_index = np.random.randint(0, len(test_generator) - 1)\n",
        "plot_3d_overlap(test_generator, res_model, random_index, density=30000, threshold=0.2)"
      ],
      "metadata": {
        "id": "26wdclFA6vkU"
      },
      "id": "26wdclFA6vkU",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30648,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 3113.603688,
      "end_time": "2024-02-04T21:49:38.772690",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2024-02-04T20:57:45.169002",
      "version": "2.5.0"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}